import requests
import re
import urllib.parse
from bs4 import BeautifulSoup

class Scanner:
    def __init__(self, url, ignore_links):
        self.session = requests.Session()
        self.target_url = url
        self.target_links = []
        self.links_to_ignore = ignore_links

    def extract_links_from(self, url, encoding='utf-8'):
        try:
            response = self.session.get(url)
            response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes
            return re.findall(r'(?:href=")(.*?)"', response.content.decode(encoding))
        except requests.RequestException as e:
            print(f"Failed to retrieve {url}: {e}")
            return []
        except UnicodeDecodeError:
            print(f"Failed to decode content from {url} using {encoding} encoding. Trying alternative encoding...")
            # Try decoding with an alternative encoding
            alternative_encodings = ['latin-1', 'iso-8859-1']
            for alt_encoding in alternative_encodings:
                try:
                    return re.findall(r'(?:href=")(.*?)"', response.content.decode(alt_encoding))
                except UnicodeDecodeError:
                    continue
            print(f"Failed to decode content from {url} using any encoding.")
            return []


    def crawl(self,url=None):
        if url==None:
            url=self.target_url
        href_links = self.extract_links_from(url)
        for link in href_links:
            link = urllib.parse.urljoin(url, link)

            if "#" in link:
                link = link.split("#")[0]

            if self.target_url in link and link not in self.target_links and link not in self.links_to_ignore:
                self.target_links.append(link)
                print(link)
                self.crawl(link)

    def extract_forms(self, url):
        response = self.session.get(url)
        parsed_html = BeautifulSoup(response.content)
        return parsed_html.findAll("form")

    def submit_form(self, form, value, url):
        action = form.get("action")  
        post_url = urllib.parse.urljoin(url, action)
        method = form.get("method")


        input_list = form.findAll("input")
        post_data={}
        for input in input_list:
            input_name = input.get("name")
            input_type = input.get("type")
            input_val = input.get("value")
            if input_type == "text":
                input_val = value

            post_data[input_name]=input_val
        if method == "post":
            return self.session.post(post_url, data = post_data)
        return self.session.get(post_url, params = post_data)
    
    def run_scanner(self):
        for link in self.target_links:
            forms = self.extract_forms(link)
            for form in forms:
                print("[+] Testing form in " + link)
                is_vulnerable_to_XSS = self.test_xss_in_form(form, link)
                if is_vulnerable_to_XSS:
                    print("\n\n[*****] XSS Discovered in " + link + " in the following form")
                    print(form)

            if "=" in link:
                print("\n\n[+] Testing " + link)
                is_vulnerable_to_XSS = self.test_xss_in_link(link)
                if is_vulnerable_to_XSS:
                    print("[*****] Discovered XSS in " + link)
            
    def test_xss_in_link(self, url):
        xss_test_script = '<scriPt>alert("Testing")</scripT>'
        url = url.replace("=", "=" + xss_test_script)
        response = self.session.get(url)
        # print(response.content)
        if xss_test_script in response.content.decode('utf-8'):
            return True
    
    def test_xss_in_form(self, form, url):
        xss_test_script = '<scriPt>alert("Testing")</scripT>'
        response = self.submit_form(form ,xss_test_script, url)
        # print(response.content)
        if xss_test_script in response.content.decode('utf-8'):
            return True

